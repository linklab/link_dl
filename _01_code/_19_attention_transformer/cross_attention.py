import torch
import torch.nn as nn

print("=" * 60)
print("Cross-Attention ì˜ˆì œ")
print("=" * 60)

print("âš ï¸  ì¤‘ìš”: nn.MultiheadAttention ì—ì„œëŠ” Queryì˜ ì…ë ¥ ì°¨ì›ì´ ë°˜ë“œì‹œ embed_dimê³¼ ì¼ì¹˜í•´ì•¼ í•¨!")
print("   kdim, vdimì€ Key, Valueì˜ ì…ë ¥ ì°¨ì›ë§Œ ì§€ì • ê°€ëŠ¥")
print("-" * 60)

seq_len_q = 10
seq_len_kv = 15
query_dim = 256
key_value_dim = 384
embed_dim = 512
num_heads = 8
batch_size = 2

print("ğŸ’¡ í•µì‹¬:")
print("   - Query ì…ë ¥ ì°¨ì› = embed_dim ì´ì–´ì•¼ í•¨")
print("   - Key/Value ì…ë ¥ ì°¨ì› = kdim, vdimìœ¼ë¡œ ì§€ì • ê°€ëŠ¥\n")

# Queryë¥¼ ë¨¼ì € íˆ¬ì˜
query_projection = nn.Linear(query_dim, embed_dim)

# Cross-Attention ìƒì„±
multi_heads_cross_attention = nn.MultiheadAttention(
    embed_dim=embed_dim,  # Query ì…ë ¥ ì°¨ì› (í•„ìˆ˜)
    num_heads=num_heads,
    kdim=key_value_dim,  # Key ì…ë ¥ ì°¨ì›
    vdim=key_value_dim,  # Value ì…ë ¥ ì°¨ì›
    batch_first=True
)

# ì…ë ¥ ìƒì„±
query = torch.randn(batch_size, seq_len_q, query_dim)
key = torch.randn(batch_size, seq_len_kv, key_value_dim)
value = torch.clone(key)

print(f"[ì›ë³¸] Query shape: {query.shape} (ì°¨ì›: {query_dim})")
print(f"Key shape: {key.shape} (ì°¨ì›: {key_value_dim})")
print(f"Value shape: {value.shape} (ì°¨ì›: {key_value_dim})")

# Query íˆ¬ì˜
projected_query = query_projection(query)
print(f"\níˆ¬ì˜ëœ Query shape: {projected_query.shape} (ì°¨ì›: {embed_dim})")

# Cross-Attention ì ìš©
output, attn_weights = multi_heads_cross_attention(
    projected_query,    # ë°˜ë“œì‹œ embed_dim ì°¨ì›
    key,                # kdimìœ¼ë¡œ ì§€ì •ëœ ì°¨ì›
    value,              # vdimìœ¼ë¡œ ì§€ì •ëœ ì°¨ì›
)

print(f"\n[ì¶œë ¥] Query shape: {output.shape}")
print(f"Attention weights shape: {attn_weights.shape}")
