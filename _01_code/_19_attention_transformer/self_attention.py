import torch
import torch.nn as nn

print("=" * 60)
print("Self-Attention ì˜ˆì œ")
print("=" * 60)

# íŒŒë¼ë¯¸í„° ì„¤ì •
seq_len = 10          # ì‹œí€€ìŠ¤ ê¸¸ì´ (Query, Key, Value ëª¨ë‘ ë™ì¼)
input_dim = 256       # ì…ë ¥ ì°¨ì›
embed_dim = 512       # Attention ë‚´ë¶€ ì„ë² ë”© ì°¨ì›
num_heads = 8         # Attention head ê°œìˆ˜
batch_size = 2        # ë°°ì¹˜ í¬ê¸°

print("\nğŸ’¡ í•µì‹¬:")
print("   - Self-Attention: Query, Key, Valueê°€ ëª¨ë‘ ë™ì¼í•œ ì…ë ¥")
print("   - Query ì…ë ¥ ì°¨ì› = embed_dim ì´ì–´ì•¼ í•¨")
print("   - ì…ë ¥ ì°¨ì›ì´ embed_dimê³¼ ë‹¤ë¥´ë©´ ë¨¼ì € íˆ¬ì˜ í•„ìš”\n")
print("-" * 60)

# ì…ë ¥ì„ embed_dimìœ¼ë¡œ íˆ¬ì˜í•˜ëŠ” ë ˆì´ì–´
input_projection = nn.Linear(input_dim, embed_dim)

# Self-Attention ìƒì„±
multi_heads_self_attention = nn.MultiheadAttention(
    embed_dim=embed_dim,      # Query, Key, Valueì˜ ì°¨ì› (ëª¨ë‘ ë™ì¼)
    num_heads=num_heads,
    batch_first=True
)

# ì…ë ¥ ìƒì„± (í•˜ë‚˜ì˜ ì‹œí€€ìŠ¤ë§Œ í•„ìš”)
input_seq = torch.randn(batch_size, seq_len, input_dim)

print(f"[ì›ë³¸] ì…ë ¥ shape: {input_seq.shape}")
print(f"  - batch_size: {batch_size}")
print(f"  - sequence_length: {seq_len}")
print(f"  - input_dim: {input_dim}")

# ì…ë ¥ì„ embed_dimìœ¼ë¡œ íˆ¬ì˜
projected_input = input_projection(input_seq)
print(f"\níˆ¬ì˜ëœ ì…ë ¥ shape: {projected_input.shape}")
print(f"  - embed_dim: {embed_dim}")

# Self-Attention ì ìš©
# Query, Key, Value ëª¨ë‘ ë™ì¼í•œ ì…ë ¥ ì‚¬ìš©
output, attn_weights = multi_heads_self_attention(
    projected_input,    # Query
    projected_input,    # Key (Queryì™€ ë™ì¼)
    projected_input,    # Value (Queryì™€ ë™ì¼)
)

print(f"\n[ì¶œë ¥] shape: {output.shape}")
print(f"  - Queryì™€ ë™ì¼í•œ shape ìœ ì§€")
print(f"\nAttention weights shape: {attn_weights.shape}")
print(f"  - ({batch_size}, {seq_len}, {seq_len})")
print(f"  - ê° í† í°ì´ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  í† í°ì— ëŒ€í•´ ê°€ì§€ëŠ” ê°€ì¤‘ì¹˜")

# Attention weights ì‹œê°í™”
import numpy as np
np.set_printoptions(precision=2, suppress=True, linewidth=150)
print(f"\n{'=' * 60}")
print("Attention Weights ì˜ˆì‹œ (ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì²˜ìŒ 10ê°œ í† í°)")
print(f"{'=' * 60}")
print(attn_weights[0, :10, :10].detach().numpy())
print("\nê° í–‰: í•´ë‹¹ í† í°ì´ ë‹¤ë¥¸ ëª¨ë“  í† í°ê³¼ ì—°ê´€ëœ ê°€ì¤‘ì¹˜")
print("ê° í–‰ì˜ í•© = 1.0 (softmax ê²°ê³¼)")
